\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{multirow}
\usepackage{bbm}

% \usepackage{kky}

\newcounter{thm}
\ifx\fact\undefined
\newtheorem{fact}[thm]{Fact}
\fi

\newcommand{\pen}{{\rm pen}}
\newcommand{\diag}{{\rm diag}}
\newcommand{\diam}{{\bf{\rm diam}}}
\newcommand{\spann}{{\bf{\rm span}}}
\newcommand{\nulll}{{\bf{\rm null}}}
% Distributions
\newcommand{\Bern}{{\bf{\rm Bern}}\,} % support of a function
\newcommand{\Categ}{{\bf{\rm Categ}}\,} % support of a function
\newcommand{\Mult}{{\bf{\rm Mult}}\,} % support of a function
\newcommand{\Dir}{{\bf{\rm Dir}}\,} % support of a function
\newcommand{\horizontalline}{\noindent\rule[0.5ex]{\linewidth}{1pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}
\newcommand{\HRuleN}{\HRule\\} 
\newcommand{\HruleN}{\Hrule\\}
\newcommand{\superscript}[1]{{\scriptsize \ensuremath{^{\textrm{#1}}}}}
\newcommand{\supindex}[2]{#1^{(#2)}}
\newcommand{\xii}[1]{\supindex{x}{#1}}
\newcommand{\yii}[1]{\supindex{y}{#1}}
\newcommand{\zii}[1]{\supindex{z}{#1}}
\newcommand{\Xii}[1]{\supindex{X}{#1}}
\newcommand{\Yii}[1]{\supindex{Y}{#1}}
\newcommand{\Zii}[1]{\supindex{Z}{#1}}
\newcommand{\NN}{\mathbb{N}} % Natural numbers
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\indfone}{\mathbbm{1}}
\newcommand{\gb}{\mathbf{g}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Db}{\mathbf{D}}
\newcommand*{\zero}{{\bf 0}}
\newcommand*{\one}{{\bf 1}}

% Stuff mostly appearing in Statistics
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}
\newcommand{\Xb}{\mathbf{X}}


%%%%  brackets
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\nbr}[1]{\left\|#1\right\|}
\newcommand{\abr}[1]{\left|#1\right|}

% derivatives and partial fractions
\newcommand{\differentiate}[2]{ \frac{ \ud #2}{\ud #1} }
\newcommand{\differentiateat}[3]{ \frac{ \ud #2}{\ud #1}  \Big|_{#1=#3} }
\newcommand{\partialfrac}[2]{ \frac{ \partial #2}{\partial #1} }
\newcommand{\partialfracat}[3]{ \frac{ \partial #2}{\partial #1} \Big|_{#1=#3} }
\newcommand{\partialfracorder}[3]{ \frac{ \partial^{#3} #2}{\partial^{#3} #1} }
\newcommand{\partialfracatorder}[4]{ \frac{ \partial^{#3} #2}{\partial^{#3} #1} \Big|_{#1=#4} }

\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}


\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\br}[1]{\{#1\}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\qedsymbol}{$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\xv}{\vc{x}}
\newcommand{\Sigmav}{\vc{\Sigma}}
\newcommand{\alphav}{\vc{\alpha}}
\newcommand{\muv}{\vc{\mu}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\x{\mathbf x}
\def\y{\mathbf y}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\E{\mathbb E}
\def\V{\mathbb V}

% TO SHOW SOLUTIONS, include following (else comment out):
\newenvironment{soln}{
	\leavevmode\color{blue}\ignorespaces
}{}

\hypersetup{
%    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3em,       % <-- and this
  headsep=2em,          % <-- and this
  footskip=3em,
}


\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 5}}
\rhead{\fancyplain{}{CS 760 Machine Learning}}
\cfoot{\thepage}

\title{\textsc{Homework 5}} % Title

%%% NOTE:  Replace 'NAME HERE' etc., and delete any "\red{}" wrappers (so it won't show up as red)

\author{
Nick Boddy \\
nboddy \\
GitHub repo: https://github.com/Nick-Boddy/CS760-HW5-Clustering-PCA \\
}

\date{}

\begin{document}

\maketitle 


\textbf{Instructions:}
Use this latex file as a template to develop your homework. Submit your homework on time as a single pdf file. Please wrap your code and upload to a public GitHub repo, then attach the link below the instructions so that we can access it. Answers to the questions that are not within the pdf are not accepted. This includes external links or answers attached to the code implementation. Late submissions may not be accepted. You can choose any programming language (i.e. python, R, or MATLAB). Please check Piazza for updates about the homework. It is ok to share the experiments results and compare them with each other.

\vspace{0.1in}


\section{Clustering}

\subsection{K-means Clustering (14 points)}

\begin{enumerate}

\item \textbf{(6 Points)}
Given $n$ observations $X_1^n = \{X_1, \dots, X_n\}$, $X_i \in \Xcal$, the K-means objective
is to find $k$
($<n$) centres $\mu_1^k = \{\mu_1, \dots, \mu_k\}$, and a rule $f:\Xcal \rightarrow
\{1,\dots, K\}$ so as to minimize the objective

\begin{equation}
J(\mu_1^K, f; X_1^n) = \sum_{i=1}^n \sum_{k=1}^K \indfone(f(X_i) = k) \|X_i - \mu_k\|^2
\label{eqn:kmeans}
\end{equation}

Let $\Jcal_K(X_1^n) = \min_{\mu_1^K, f} J(\mu_1^K, f; X_1^n)$. Prove that
$\Jcal_{K}(X_1^n)$ is a non-increasing function of $K$.

\begin{soln}
	$$\Jcal_K(X^n) = \min_{\mu^K, f} J(\mu^K, f; X^n)$$
	$$\Jcal_K(X^n) = \min_{\mu^K, f} \sum_{i=1}^n \sum_{k=1}^K \indfone(f(X_i) = k) \|X_i - \mu_k\|^2$$
	Conceptually speaking, when given K, we wish to find the K centroids that best represent our data $X^n$. By best representation, we mean to minimize the squared distance of every data point to the centroid it is assigned. Logically, it makes sense to assign each data point to its nearest centroid then (this will be our $f$). \\ \\
	Thus, now knowing how our rule $f$ should map $\Xcal$ to $\mathcal{K}$, we must consider how to minimize our objective with the centroids $\mu^K$. Since we're mapping each $X$ to the nearest $\mu \in \mu^K$, and our objective is proportional to the sum of the squared distances of all $X$ to their respective $\mu$, the $\mu^K$ that minimizes our objective is the set of centroids such that each centroid is at the center of the $X$ belonging to it. And so, minimizing the objective gives us clusters, of a sort. \\ \\
	With an increasing $K$, we can divide the data further into more clusters of smaller sizes. With this, our centroids will be closer its data points than they were with smaller $K$.
\end{soln}

\item \textbf{(8 Points)}
Consider the K-means (Lloyd's) clustering algorithm we studied in class. We
terminate the algorithm when there are no changes to the objective.
Show that the algorithm terminates in a finite number of steps.

\begin{soln}
	When $X^n$ is a finite set, and we then have a finite number of clusters (since $k < n$), there is a finite set of possible assignments of data points in $X$ to clusters in $\mu^k$. \\
	By the nature of the K-means algorithm (alternating between assigning each $x_i$ to its nearest centroid in $\mu^k$ and recalculating the mean of each centroid), we know that the algorithm must terminate in a finite number of steps because: \\
	1. The objective function, if it changes, decreases due to finding a closer centroid for at least one data point. \\
	2. The objective function is non-increasing, so if it does change, it decreases. \\
	3. If no change to the objective function occurs after an iteration, the program terminates. \\
	4. There is a finite number of possible centroid assignments, and therefore a finite number of changes to the objective function. \\
	
	Due to these reasons, the K-means clustering algorithm, under the given parameters, will terminate in a finite number of steps.
\end{soln}

\end{enumerate}


\subsection{Experiment (20 Points)}

In this question, we will evaluate
K-means clustering and GMM on a simple 2 dimensional problem.
First, create a two-dimensional synthetic dataset of 300 points by sampling 100 points each from the
three Gaussian distributions shown below:
\[
P_a = \Ncal\left(
\begin{bmatrix}
-1 \\ -1
\end{bmatrix},
\;
\sigma\begin{bmatrix}
2, &0.5 \\ 0.5, &1
\end{bmatrix}
\right),
\quad
P_b = \Ncal\left(
\begin{bmatrix}
1 \\ -1
\end{bmatrix},
\;
 \sigma\begin{bmatrix}
1, &-0.5 \\ -0.5, &2
\end{bmatrix}
\right),
\quad
P_c = \Ncal\left(
\begin{bmatrix}
0 \\ 1
\end{bmatrix},
\;
 \sigma\begin{bmatrix}
1 &0 \\ 0, &2
\end{bmatrix}
\right)
\]
Here, $\sigma$ is a parameter we will change to produce different datasets.\\

First implement K-means clustering and the expectation maximization algorithm for GMMs.
Execute both methods on five synthetic datasets,
generated as shown above with $\sigma \in \{0.5, 1, 2, 4, 8\}$. Finally, evaluate both methods on \emph{(i)} the clustering objective~\eqref{eqn:kmeans} and \emph{(ii)}  the clustering accuracy. For each of the two criteria, plot the value achieved by each method against $\sigma$.\\


Guidelines:
\begin{itemize} 
\item Both algorithms are only guaranteed to find only a local optimum so we recommend trying multiple
restarts and picking the one with the lowest objective value (This is~\eqref{eqn:kmeans} for K-means and the negative log likelihood for GMMs).
You may also experiment with a smart initialization
strategy (such as kmeans++).

\item
To plot the clustering accuracy,  you may treat the `label' of points generated from distribution
$P_u$ as $u$, where $u\in \{a, b, c\}$.
Assume that the cluster id $i$ returned by a method is $i\in \{1, 2, 3\}$.
Since clustering is an unsupervised learning problem, you should obtain the best possible mapping
from $\{1, 2, 3\}$ to $\{a, b, c\}$ to compute the clustering objective.
One way to do this is to compare the clustering centers returned by the method (centroids for
K-means, means for GMMs) and map them to the distribution with the closest mean.

\end{itemize}

Points break down: 7 points each for implementation of each method, 6 points for reporting of
evaluation metrics.
\\ \\
\begin{soln}
	With a $\sigma$ of 0.5, both K-means and GMM achieve about $80\%$ accuracy and taper down 
	K-means clustering seems to perform a bit better than GMM on these generated datasets, but don't differ too much. GMM definitely seems to be more inconsistent and more prone to fall into local minima. I'm not sure as to the best ways to initialize the latent variables, but that could help with its performance for sure. \\
	\includegraphics[width=3in]{acc_vs_var.png} \\
	\includegraphics[width=3in]{obj_vs_var.png} \\
	\includegraphics[width=3in]{nll_vs_var.png} \\
\end{soln}

\section{Linear Dimensionality Reduction}

\subsection{Principal Components Analysis  (10 points)}
\label{sec:pca}

Principal Components Analysis (PCA) is a popular method for linear dimensionality reduction. PCA attempts to find a lower dimensional subspace such that when you project the data onto the subspace as much of the information is preserved. Say we have data $X = [x_1^\top; \dots; x_n^\top] \in \RR^{n\times D}$ where  $x_i \in \RR^D$. We wish to find a $d$ ($ < D$) dimensional subspace $A = [a_1, \dots, a_d] \in \RR^{D\times d}$, such that $ a_i \in \RR^D$ and $A^\top A = I_d$, so as to maximize $\frac{1}{n} \sum_{i=1}^n \|A^\top x_i\|^2$.
\begin{enumerate}

\item  \textbf{(4 Points)}
Suppose we wish to find the first direction $a_1$ (such that $a_1^\top a_1 = 1$) to maximize $\frac{1}{n} \sum_i (a_1^\top x_i)^2$.
Show that $a_1$ is the first right singular vector of $X$.

\begin{soln}
	$$a_1 = \arg \max_{a_1} \frac{1}{n} \sum_{i=1}^n (a_1^T x_i)^2$$
	$$ = \arg \max_{a_1} \frac{1}{n} \sum_{i=1}^n a_1^T x_i x_i^T a_1$$
	$$ = \arg \max_{a_1} a_1^T \left( \frac{1}{n} \sum_{i=1}^N x_i x_i^T \right) a_1$$
	$$ = \arg \max_{a_1} \frac{1}{n} a_1^T X^T X a_1$$
	Since $\frac{1}{n}$ is constant, we have
	$$a_1 = \arg \max_{a_1} a_1^T X^T X a_1$$
	By SVD, $X = U \Sigma V^T$ so 
	$$X^T X = (U \Sigma V^T)^T(U \Sigma V^T)$$
	$$ = (V \Sigma^T U^T)(U \Sigma V^T)$$
	$$ = V \Sigma^T \Sigma V^T \text{ since } U^T U = I$$
	Substituting, we now have
	$$a_1 = \arg \max_{a_1} a_1^T V \Sigma^T \Sigma V^T a_1$$
	Because $V$ is an orthogonal matrix and $V^\top V = I$, it doesn't affect our optimization constraint.
	$$a_1 = \arg \max_{a_1} a_1^T \Sigma^T \Sigma a_1$$
	$\Sigma^T \Sigma$ is the diagonal matrix of squared singular values: $\text{diag}(\sigma_1^2, \sigma_2^2, \ldots ,\sigma_D^2)$
	such that $\sigma_1^2 \geq \sigma_2^2 \geq \ldots \geq \sigma_D^2$.
	
	Maximizing $a_1^T \Sigma^T \Sigma a_1$ is therefore the same as maximizing $\sigma_1^2 a_1^T a_1$,
	
	Since $a_1^T a_1 = 1$, the optimal $a_1$ is the one in which $a_1$ is the unit vector that maximizes the term of $\sigma_1^2$, the first squared singular value. This $a_1$ must therefore be the first right singular vector.
\end{soln}

\item  \textbf{(6 Points)}
Given $a_1, \dots, a_k$, let $A_k = [a_1, \dots, a_k]$ and 
$\tilde{x}_i = x_i - A_kA_k^\top x_i$. We wish to find $a_{k+1}$, to maximize
$\frac{1}{n} \sum_i (a_{k+1}^\top \tilde{x}_i)^2$. Show that $a_{k+1}$ is the
$(k+1)^{th}$ right singular vector of $X$.

\begin{soln}
	Since $A_k A_k^\top$ is the projection matrix made by the first $k$ principal components / right singular vectors, $A_k A_k^\top x_i$ is the projection of $x_i$ onto the subspace spanned by $A_k = [a_1, \dots, a_k]$. \\
	Therefore, $\tilde{x} = x_i - A_k A_k^\top x_i$ is the component of $x_i$ that is orthogonal to that subspace. Similarly, $\tilde{X} = X - A_k A_k^\top X$, the residual matrix, is the part of $X$ orthogonal to the subspace.
	
	$$a_{k+1} = \arg \max_{a_{k+1}} \frac{1}{n} \sum_{i=1}^{n} (a_{k+1}^\top \tilde{x}_i)^2$$
	$$ = \arg \max_{a_{k+1}} \frac{1}{n} \sum_{i=1}^{n} (a_{k+1}^\top \tilde{x}_i)(\tilde{x}_i^\top a_{k+1})$$
	$$ = \arg \max_{a_{k+1}} \frac{1}{n} a_{k+1}^\top \left( \sum_{i=1}^{n} \tilde{x}_i \tilde{x}_i^\top\right) a_{k+1}$$
	$$ = \arg \max_{a_{k+1}} \frac{1}{n} a_{k+1}^\top \tilde{X}^\top \tilde{X} a_{k+1}$$
	Since $\frac{1}{n}$ is a constant, we have
	$$a_{k+1} = \arg \max_{a_{k+1}} a_{k+1}^\top \tilde{X}^\top \tilde{X} a_{k+1}$$
	From a similar process of part 1 above, we use SVD of $\tilde{X}$ to arrive at
	$$a_{k+1} = \arg \max_{a_{k+1}} \tilde{\sigma}_1^2 a_{k+1}^\top a_{k+1}$$
	where $\tilde{\sigma}_1^2$ is the first (and largest) squared singular value of the residual matrix. The unit vector $a_{k+1}$ that maximizes this is the corresponding to the first column of $\tilde{V}$, and thus the first right singular vector of $\tilde{X}$. And since $\tilde{X}$ is the residual matrix, the portion of $X$ orthogonal to the subspace spanned by $A_k$, this $a_{k+1}$ is the $(k+1)^{th}$ right singular vector of $X$.
\end{soln}

\end{enumerate}


\subsection{Dimensionality reduction via optimization (22 points)}

We will now motivate the dimensionality reduction problem from a slightly different
perspective. The resulting algorithm has many similarities to PCA.
We will refer to method as DRO.

As before, you are given data $\{x_i\}_{i=1}^n$, where $x_i \in \RR^D$. Let $X=[x_1^\top; \dots
x_n^\top] \in \RR^{n\times D}$. We suspect that the data
actually lies approximately in  a $d$ dimensional affine subspace.
Here $d<D$ and $d<n$.
Our goal, as in PCA, is to use this dataset to find a $d$ dimensional representation $z$ for each $x\in\RR^D$.
(We will assume that the span of the data has dimension larger than
$d$, but our method should work whether $n>D$ or $n<D$.)


Let $z_i\in \RR^d$ be the lower dimensional representation for $x_i$ and
let $Z = [z_1^\top; \dots; z_n^\top] \in \RR^{n\times d}$.
We wish to find parameters $A \in \RR^{D\times d}$, $b\in\RR^D$ and the lower
dimensional representation $Z\in \RR^{n\times d}$ so as to minimize 
\begin{equation}
J(A,b,Z) = \frac{1}{n} \sum_{i=1}^n \|x_i - Az_i - b\|^2 = \| X - ZA^\top - \one b^\top\|_F^2.
\label{eqn:dimobj}
\end{equation}
Here, $\|A\|^2_F = \sum_{i,j} A_{ij}^2$ is the Frobenius norm of a matrix.


\begin{enumerate}
\item \textbf{(3 Points)}
Let $M\in\RR^{d\times d}$ be an arbitrary invertible matrix and $p\in\RR^{d}$ be an arbitrary vector.
Denote, $A_2 = A_1M^{-1}$, $b_2 = b_1- A_1M^{-1}p$ and $Z_2 = Z_1 M^\top +
\one p^\top$.
Show that both
$(A_1, b_1, Z_1)$ and $(A_2, b_2, Z_2)$ achieve the same objective value $J$~\eqref{eqn:dimobj}.
\end{enumerate}

\begin{soln}
	$$
	\begin{aligned}
		J(A_2, b_2, Z_2) = \frac{1}{n} \| X - Z_2 A_2^\top - 1 b_2^\top \|_F^2 
		&= \frac{1}{n} \| X - (Z_1 M^\top + \one p^\top)(A_1 (M^{-1})^\top) - \one (b_1 - A_1 M^{-1} p)^\top \|_F^2 \\
		&= \frac{1}{n} \| X - (Z_1 M^\top + \one p^\top)((M^{-1})^\top A_1^\top) - \one (b_1^\top - p^\top (M^{-1})^\top A_1^\top) \|_F^2 \\
		&= \frac{1}{n} \| X - (Z_1 M^\top (M^{-1})^\top A_1^\top + \one p^\top (M^{-1})^\top A_1^\top) - (\one b_1^\top - \one p^\top (M^{-1})^\top A_1^\top) \|_F^2 \\
		&= \frac{1}{n} \| X - Z_1 A_1^\top - \one p^\top (M^{-1})^\top A_1^\top - \one b_1^\top + \one p^\top (M^{-1})^\top A_1^\top \|_F^2 \\
		&= \frac{1}{n} \| X - Z_1 A_1^\top - \one b_1^\top \|_F^2 \\
		&= J(A_1, b_1, Z_1)
	\end{aligned}
	$$
\end{soln}

Therefore, in order to make the problem determined, we need to impose some
constraint on $Z$. We will assume that the $z_i$'s have zero mean and identity covariance.
That is,
\begin{align*}
\Zbar = \frac{1}{n} \sum_{i=1}^n z_i =\frac{1}{n} Z^\top {\bf 1}_n = 0, \hspace{0.3in} 
S = \frac{1}{n} \sum_{i=1}^n z_i z_i^\top 
= \frac{1}{n} Z^\top Z
= I_d
\end{align*}
Here, ${\bf 1}_d = [1, 1 \dots, 1]^\top \in\RR^d$ and $I_d$  is the $d\times d$ identity matrix.

\begin{enumerate}
\setcounter{enumi}{1}
\item \textbf{(16 Points)}
Outline a procedure to solve the above problem. Specify how you
would obtain $A, Z, b$ which minimize the objective and satisfy the constraints.

\textbf{Hint: }The rank $k$ approximation of a matrix in Frobenius norm is obtained by
taking its SVD and then zeroing out all but the first $k$ singular values.

\begin{soln}
	$$A, Z, b = \arg \min_{A, Z, b} \left( \frac{1}{n} \| X - ZA^\top - \one b^\top\|_F^2 \right)$$
	$$ = \arg \min_{A, Z, b} \left( \frac{1}{n} \sum_{i=1}^n \|x_i - Az_i - b\|^2 \right)$$
	Let's first consider what value $b$ might take on when minimizing this $J$ function.
	$$\frac{\partial}{\partial b} \left( \frac{1}{n} \sum_{i=1}^n \|x_i - Az_i - b\|^2 \right) = 0$$
	$$\frac{\partial}{\partial b} \left( \frac{1}{n} \sum_{i=1}^n (x_i - Az_i - b)^\top (x_i - Az_i - b) \right) = 0$$
	$$\frac{\partial}{\partial b} \left( \frac{1}{n} \sum_{i=1}^n (x_i - Az_i)^\top (x_i - Az_i) - 2(x_i - Az_i)^\top b + b^\top b \right) = 0$$
	$$\frac{1}{n} \sum_{i=1}^n \frac{\partial}{\partial b} \left((x_i - Az_i)^\top (x_i - Az_i) - 2(x_i - Az_i)^\top b + b^\top b \right) = 0$$
	$$\frac{1}{n} \sum_{i=1}^n -2(x_i - Az_i) + 2b = 0$$
	$$\frac{1}{n} \sum_{i=1}^n b - (x_i - Az_i) = 0$$
	$$\frac{1}{n} \sum_{i=1}^n b - \frac{1}{n} \sum_{i=1}^n x_i + \frac{1}{n} \sum_{i=1}^n Az_i = 0$$
	Under the constraints that $z_i$'s have zero mean and identity covariance, this becomes
	$$n \cdot b = \sum_{i=1}^n x_i$$
	$$b = \frac{1}{n} \sum_{i=1}^n x_i$$
	In other words, b ought to be the mean of $x_i$'s.
	\begin{enumerate}
		\item First, set $b = \frac{1}{n} \sum_{i=1}^{n} x_i$
		\item With $b$ as a $D \times 1$ matrix, subtract $\one_n b^\top$ from $X$ to get $\tilde{X}$. \\
		This $\tilde{X}$ will be mean-centered around zero (and is therefore aligned with what we want $Z$ to be).
		\item To minimize our function $\frac{1}{n} \|\tilde{X} - Z A^\top \|_F^2$, we want $Z A^\top = \tilde{X}$
		\item Deconstruct $\tilde{X}$ using SVD. \\ $\tilde{X} = U \Sigma V^\top$
		\item We now want to correspond $Z$ with $U \Sigma$ and $A$ with $V$.
		\item We do this by taking the $d$ approximation of $\tilde{X}$. \\
		Let $U_d$ be $n \times d$, $\Sigma_d$ be $d \times d$, and $V_d$ be $D \times d$.
		$U_d \Sigma_d V_d^\top$ is the rank $d$ approximation of $\tilde{X}$.
		\item Approximate $Z = U_d \Sigma_d$ and $A=V_d$. \\
		To satisfy our constraints that $Z$ has zero mean and identity covariance, we should rescale $Z$. After rescaling, we should then re-estimate $A$ to minimize our $J$ function. We may afterward want to iterate this process of estimating and scaling $Z$, and estimating $A$.
	\end{enumerate}
\end{soln}

\item \textbf{(3 Points)}
You are given a point $x_*$ in the original $D$ dimensional space.
State the rule to obtain the $d$ dimensional
representation $z_*$ for this new point.
(If $x_*$ is some original point $x_i$ from the $D$--dimensional space, it should be the
$d$--dimensional representation $z_i$.)

\begin{soln}
	After our process of approximating our parameters $Z$, $A$, and $b$, when given $x_\star$, which is $D \times 1$, we can do the following to calculate $z_\star$:
	\begin{enumerate}
		\item Subtract $b$ from $x_\star$. \\
		$\tilde{x}_\star = x_\star - b$
		\item Project $\tilde{x}_\star$ onto the $d$ dimensional space by multiplying by $A$. This will be $z_\star$. \\
		$z_\star = A^\top \tilde{x}_\star$
	\end{enumerate}
\end{soln}


\end{enumerate}


\subsection{Experiment (34 points)}

Here we will compare the above three methods on two data sets. 

\begin{itemize}
\item We will implement three variants of PCA:
\begin{enumerate}
    \item "buggy PCA": PCA applied directly on the matrix $X$.
    \item "demeaned PCA": We subtract the mean along each dimension before applying PCA.
    \item "normalized PCA": Before applying PCA, we subtract the mean and scale each dimension so that the sample  mean and standard deviation along each dimension is $0$ and $1$ respectively.
    
\end{enumerate}


\item 
One way to study how well the low dimensional representation $Z$ captures the linear
structure in our data is to project $Z$ back to $D$ dimensions and look at the reconstruction
error. For PCA, if we mapped it to $d$ dimensions via $z = Vx$ then the
reconstruction is $V^\top z$. For the preprocessed versions, we first do this and then
reverse the preprocessing steps as well. For DRO  we just compute $Az + b$.
We will compare all methods by the reconstruction error on the datasets.

\item 
Please implement code for the methods: Buggy PCA (just take the SVD of $X$)
, Demeaned PCA,
Normalized PCA, DRO. In all cases your function should take in
an $n \times d$ data matrix and $d$ as an argument. It should return the
the $d$ dimensional representations, the estimated parameters, and the
reconstructions of these representations in $D$ dimensions. 

\item
You are given two datasets: A two Dimensional dataset with $50$ points 
\texttt{data2D.csv} and a thousand dimensional dataset with $500$ points
\texttt{data1000D.csv}. 

\item
For the $2D$ dataset use $d=1$. For the $1000D$ dataset, you need to choose
$d$. For this, observe the singular values in DRO and see if there is a clear
``knee point" in the spectrum.
Attach any figures/ Statistics you computed to justify your choice.

\item
For the $2D$ dataset you need to attach the a 
plot comparing the orignal points with the reconstructed points for all 4
methods.
For both datasets you should also report the reconstruction errors, that is the squared sum of
differences $\sum_{i=1}^n \|x_i - r(z_i)\|^2$,
where $x_i$'s are the original points and $r(z_i)$ are the $D$ dimensional points
reconstructed from the 
$d$ dimensional representation $z_i$.

\begin{soln}
	When applying my implementations on the 2D dataset, I get the following results. \\
	\includegraphics[width=3in]{2d_buggy}
	\includegraphics[width=3in]{2d_demeaned}
	\includegraphics[width=3in]{2d_normalized}
	\includegraphics[width=3in]{2d_dro}
	\begin{center}
		data2D: 
		\begin{tabular}{ c  c }
			Algorithm & Reconstruction Error \\
			\hline
			Buggy PCA & 44.345154 \\
			Demeaned PCA & 0.500304 \\
			Normalized PCA & 2.473604 \\
			DRO & 0.500304 \\
		\end{tabular}
	\end{center}
	
	Both demeaned PCA and DRO have the lowest errors. \\
	Now, when given the 1000 dimensional dataset, to choose a reasonable $d$, I used a portion of my DRO strategy. I subtracted the mean from the data and performed Singular Value Decomposition. I then looked at the resultant diagonal (singular values) of $\Sigma$, which are of decreasing order. Plotting a bar chart shows an obvious "knee point": \\
	\includegraphics[width=3in]{sigma_500} \\
	A closer look: \\
	\includegraphics[width=3in]{sigma_50} \\
	What we can infer is that only the first $30$ singular values are of significant importance if we wish to approximate our data. \\
	Therefore, I choose $d = 30$. Any more dimensions is unnecessary. \\ \\
	When applying the implementations to the 1000D dataset with $d = 30$, I get the following results: \\
	\begin{center}
		data1000D: 
		\begin{tabular}{ c  c }
			Algorithm & Reconstruction Error \\
			\hline
			Buggy PCA & 401365.699 \\
			Demeaned PCA & 136522.979 \\
			Normalized PCA & 136814.290 \\
			DRO & 136522.979 \\
		\end{tabular}
	\end{center}
\end{soln}

\item \textbf{Questions:} After you have completed the experiments, please answer the following questions.
\begin{enumerate}
\item Look at the results for Buggy PCA. The reconstruction error is bad and the
reconstructed points don't seem to well represent the original points. Why is
this? \\
\textbf{Hint: } Which subspace is Buggy PCA trying to project the points
onto?

\begin{soln}
	The reason why Buggy PCA performs so poorly is solely due to the reason that it does not demean, or center, the data before finding the principal components. Since PCA attempts to capture the directions of the most variance, if the data has a significantly non-zero mean, the first "axis" of high variance that is captured could potentially the direction of the mean, if influential enough. \\
	So, in a sense, Buggy PCA is trying to project onto the subspace aligned in the approximate direction of the mean.
\end{soln}

\item The error criterion we are using is the average squared error 
between the original points and the reconstructed points.
In both examples DRO and demeaned PCA achieves the lowest error among all
methods. 
Is this surprising? Why?

\begin{soln}
	I think the error criterion we're using isn't the average squared error, but rather the sum of the squared errors. \\
	However, it doesn't impact our observation that DRO and demeaned PCA achieved the lowest reconstruction errors for both datasets. This isn't terribly surprising a result because, as mathematically derived previously, these are the best methods to minimize our objective functions. With DRO, we determined that it's optimal to subtract $b = \frac{1}{n} \sum_{i=1}^{n} x_i$ from $X$. This is equivalent to demeaning our data, as is done in both DRO and demeaned PCA. After demeaning, both algorithms then deconstruct using SVD. \\
	In the case of demeaned PCA, we take the $d$ right singular vectors of $\tilde{X}$ and transform our data with $X_d = \tilde{X} A$. This essentially equates $X_d$ with the $U$ and $\Sigma$ components of SVD, which is what is done in DRO. So because of this correspondence, it makes sense that they both achieve near-identical results.
\end{soln}

\end{enumerate}

\item Point allocation:
\begin{itemize}
\item Implementation of the three PCA methods: \textbf{(6 Points)}
\item Implementation of DRO: \textbf{(6 points)}
\item Plots showing original points and reconstructed points for 2D dataset for each one of the 4 methods: \textbf{(10 points)}
\item Implementing reconstructions and reporting results for each one of the 4 methods for the 2 datasets: \textbf{(5 points)}
\item Choice of $d$ for $1000D$ dataset and appropriate justification:
\textbf{(3 Points)}
\item Questions \textbf{(4 Points)}
\end{itemize}

\end{itemize}


%\vspace{0.1in}

\vspace{0.2in}

\textbf{Answer format:}  \\
The graph bellow is in example of how a plot of one of the algorithms for the 2D dataset may look like: \\
\includegraphics[width=3in]{buggy_pca} \hspace{0.4in}
\\

The blue circles are from the original dataset and the red crosses are the reconstructed points. \\

And this is how the reconstruction error may look like for Buggy PCA for the 2D dataset: 0.886903







\bibliographystyle{apalike}
\end{document}


